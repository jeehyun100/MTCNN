{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DQN.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jeehyun100/MTCNN/blob/master/DQN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "am5VU5sJUqxo",
        "colab_type": "text"
      },
      "source": [
        "# Python Libray Import"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kk9i3FCNdkwq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 63
        },
        "outputId": "476da098-9e8e-4b97-9cc5-218d927e8a25"
      },
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import random\n",
        "import gym\n",
        "from collections import deque"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WsjiblEIUmMs",
        "colab_type": "text"
      },
      "source": [
        "# Colab drive Mount\n",
        "### model file 저장하기 위해 본인의 google drive와 연결"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rqqDqziPHEEn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "6321b4d3-af07-4d2d-b9a3-9bee4a2e6331"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VuAPj9QEVH_p",
        "colab_type": "text"
      },
      "source": [
        "# Openai Gym 및 ipython display Setting\n",
        "## colab에서 gym 환경을 plotting해서 볼수 있게 설정"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j2u6k8GkkCjG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "4a61aa18-fe0c-454a-91c2-c26b316ede7e"
      },
      "source": [
        "!apt-get install -y xvfb python-opengl > /dev/null 2>&1\n",
        "!pip install gym pyvirtualdisplay > /dev/null 2>&1\n",
        "\n",
        "!apt-get install x11-utils > /dev/null 2>&1 and !pip install pyglet==v1.3.\n",
        "!pip install gym --upgrade > /dev/null 2>&1\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython import display as ipythondisplay\n",
        "from pyvirtualdisplay import Display\n",
        "display = Display(visible=0, size=(400, 300))\n",
        "display.start()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "xdpyinfo was not found, X start can not be checked! Please install xdpyinfo!\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<Display cmd_param=['Xvfb', '-br', '-nolisten', 'tcp', '-screen', '0', '400x300x24', ':1001'] cmd=['Xvfb', '-br', '-nolisten', 'tcp', '-screen', '0', '400x300x24', ':1001'] oserror=None return_code=None stdout=\"None\" stderr=\"None\" timeout_happened=False>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KI3TCY1AlHSp",
        "colab_type": "code",
        "outputId": "4eed16f2-0750-419f-a144-11dc44053b8f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "!pip install pyglet"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pyglet in /usr/local/lib/python3.6/dist-packages (1.3.2)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from pyglet) (0.16.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ariPA1qyL9Xa",
        "colab_type": "text"
      },
      "source": [
        "# Cartpole 문제정의\n",
        "## 이 게임은 손 위에 막대를 세워놓고, 막대가 쓰러지지 않도록 하는 게임입니다.\n",
        "\n",
        "ㆍObservation : [x, θ, dx/dt, dθ/dt]\n",
        "- x : track 상에서 cart의 위치\n",
        "- θ : pole과 normal line과의 각도\n",
        "- dx/dt : cart의 속도\n",
        "- dθ/dt : θ의 각속도\n",
        "\n",
        "ㆍEnding condition(of episode)\n",
        " 1.   θ가 15˚이상\n",
        " 2.   원점(O: cetroid of track)으로부터의 거리가 2.4 units이상 \n",
        "\n",
        "ㆍAction : cart의 가하는 힘의 방향 (0 or 1)\n",
        "\n",
        "ㆍReward : episode가 유지되는 시간\n",
        "\n",
        "ㆍObjective : Ending condition을 피하며 reward를 최대로(pole의 균형을 오랫동안 유지)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QJzgZtWZLKYj",
        "colab_type": "code",
        "outputId": "87e673d1-89da-4809-e843-95ab71316331",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 337
        }
      },
      "source": [
        "from pyvirtualdisplay import Display\n",
        "Display().start()\n",
        "\n",
        "import gym\n",
        "from IPython import display\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "env = gym.make('CartPole-v0')\n",
        "state = env.reset()\n",
        "img = plt.imshow(env.render('rgb_array'))\n",
        "with tf.Session() as sess:\n",
        "\n",
        "    #mainDQN = DQN(sess, INPUT_SIZE, OUTPUT_SIZE)\n",
        "    init = tf.global_variables_initializer()\n",
        "    #mainDQN.load(\"dqn_v1\")\n",
        "    for _ in range(50):\n",
        "        img.set_data(env.render('rgb_array')) # just update the data\n",
        "        display.display(plt.gcf())\n",
        "        display.clear_output(wait=True)\n",
        "        action = env.action_space.sample()\n",
        "        #action = np.argmax(mainDQN.predict(state))\n",
        "        print()\n",
        "        next_state, reward, done, _ = env.step(action)\n",
        "        state = next_state\n",
        "        print(\"action, state : {0} {1}\".format(action,state ))\n",
        "print(\"Input space {0}\".format(env.observation_space.shape[0]))\n",
        "print(\"Output space(Action) {0}\".format(env.action_space.n))\n"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "action, state : 0 [ 0.72691261  0.78946532 -2.70350399 -8.2303533 ]\n",
            "Input space 4\n",
            "Output space(Action) 2\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAW4AAAD8CAYAAABXe05zAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X8McqAVdd1tJ",
        "colab_type": "text"
      },
      "source": [
        "# Make Deep Q Network\n",
        "## Cart pole을 훈련 시킬 Deep Q Network를 만든다.\n",
        "\n",
        "ㆍ_build_network\n",
        "- Deep Nueral Network를 만드는 메소드\n",
        "- input size : 4 -> hidden node size : 16 -> action size : 2\n",
        "\n",
        "ㆍupdate\n",
        "- Deep Learning Network를 훈련시키는 메소드\n",
        "\n",
        "ㆍpredict\n",
        "- state를 넣으면 action을 반환해주는 메소드"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fzM0zrMNdbwQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"DQN Class\n",
        "\n",
        "DQN(NIPS-2013)\n",
        "\"Playing Atari with Deep Reinforcement Learning\"\n",
        "https://www.cs.toronto.edu/~vmnih/docs/dqn.pdf\n",
        "\n",
        "DQN(Nature-2015)\n",
        "\"Human-level control through deep reinforcement learning\"\n",
        "http://web.stanford.edu/class/psych209/Readings/MnihEtAlHassibis15NatureControlDeepRL.pdf\n",
        "\"\"\"\n",
        "\n",
        "class DQN:\n",
        "\n",
        "    def __init__(self, session: tf.Session, input_size: int, output_size: int, name: str=\"main\") -> None:\n",
        "        \"\"\"DQN Agent can\n",
        "\n",
        "        1) Build network\n",
        "        2) Predict Q_value given state\n",
        "        3) Train parameters\n",
        "\n",
        "        Args:\n",
        "            session (tf.Session): Tensorflow session\n",
        "            input_size (int): Input dimension\n",
        "            output_size (int): Number of discrete actions\n",
        "            name (str, optional): TF Graph will be built under this name scope\n",
        "        \"\"\"\n",
        "        self.session = session\n",
        "        self.input_size = input_size\n",
        "        self.output_size = output_size\n",
        "        self.net_name = name\n",
        "        \n",
        "\n",
        "        self._build_network()\n",
        "        self.saver = tf.train.Saver()\n",
        "\n",
        "    def _build_network(self, h_size=16, l_rate=0.001) -> None:\n",
        "        \"\"\"DQN Network architecture (simple MLP)\n",
        "\n",
        "        Args:\n",
        "            h_size (int, optional): Hidden layer dimension\n",
        "            l_rate (float, optional): Learning rate\n",
        "        \"\"\"\n",
        "        with tf.variable_scope(self.net_name) as scope:\n",
        "\n",
        "            self._X = tf.placeholder(tf.float32, [None, self.input_size], name=\"input_x\")\n",
        "            net = self._X\n",
        "\n",
        "            net = tf.layers.dense(net, h_size, activation=tf.nn.relu)\n",
        "            net = tf.layers.dense(net, self.output_size)\n",
        "            self._Qpred = net\n",
        "\n",
        "            self._Y = tf.placeholder(tf.float32, shape=[None, self.output_size])\n",
        "            self._loss = tf.losses.mean_squared_error(self._Y, self._Qpred)\n",
        "\n",
        "            optimizer = tf.train.AdamOptimizer(learning_rate=l_rate)\n",
        "            self._train = optimizer.minimize(self._loss)\n",
        "            scope.reuse_variables()\n",
        "\n",
        "    def predict(self, state: np.ndarray) -> np.ndarray:\n",
        "        \"\"\"Returns Q(s, a)\n",
        "\n",
        "        Args:\n",
        "            state (np.ndarray): State array, shape (n, input_dim)\n",
        "\n",
        "        Returns:\n",
        "            np.ndarray: Q value array, shape (n, output_dim)\n",
        "        \"\"\"\n",
        "        x = np.reshape(state, [-1, self.input_size])\n",
        "        return self.session.run(self._Qpred, feed_dict={self._X: x})\n",
        "\n",
        "    def update(self, x_stack: np.ndarray, y_stack: np.ndarray) -> list:\n",
        "        \"\"\"Performs updates on given X and y and returns a result\n",
        "\n",
        "        Args:\n",
        "            x_stack (np.ndarray): State array, shape (n, input_dim)\n",
        "            y_stack (np.ndarray): Target Q array, shape (n, output_dim)\n",
        "\n",
        "        Returns:\n",
        "            list: First element is loss, second element is a result from train step\n",
        "        \"\"\"\n",
        "        feed = {\n",
        "            self._X: x_stack,\n",
        "            self._Y: y_stack\n",
        "        }\n",
        "        return self.session.run([self._loss, self._train], feed)\n",
        "     \n",
        "    def save(self, file_path):\n",
        "        \"\"\"save \n",
        "\n",
        "        \"\"\"\n",
        "        self.saver.save(self.session, file_path)\n",
        "        #saver.save(sess, checkpoint_path, global_step=step)\n",
        "    def load(self, file_path):\n",
        "        \"\"\"save \n",
        "\n",
        "        \"\"\"\n",
        "        self.saver.restore(sess, file_path)\n",
        "        print(\"load complete\")\n",
        "        #self.saver.save(self.session, file_path)\n",
        "        #saver.save(sess, checkpoint_path, global_step=step)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jv3BEY7KeZbg",
        "colab_type": "text"
      },
      "source": [
        "# Initialize Parameters\n",
        "\n",
        "## Openai gym 훈련용 환경을 만들고, 여러가지 기초정보를 세팅한다.\n",
        "\n",
        "ㆍ DISCOUNT_RATE\n",
        "\n",
        "ㆍ REPLAY_MEMORY : 훈련 경험을 저장하는 공간\n",
        "\n",
        "ㆍ MAX_EPISODE : 훈련 횟수\n",
        "\n",
        "ㆍ BATCH_SIZE : 한번에 훈련하는(BATCH) 단위(SIZE)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ts_hU38keQeP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "env = gym.make('CartPole-v0')\n",
        "env = gym.wrappers.Monitor(env, 'gym-results/', force=True)\n",
        "INPUT_SIZE = env.observation_space.shape[0]\n",
        "OUTPUT_SIZE = env.action_space.n\n",
        "\n",
        "DISCOUNT_RATE = 0.99\n",
        "REPLAY_MEMORY = 50000\n",
        "MAX_EPISODE = 10\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "# minimum epsilon for epsilon greedy\n",
        "MIN_E = 0.0\n",
        "# epsilon will be `MIN_E` at `EPSILON_DECAYING_EPISODE`\n",
        "EPSILON_DECAYING_EPISODE = MAX_EPISODE * 0.01"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iuz46GrVddwI",
        "colab_type": "text"
      },
      "source": [
        "# 훈련 준비 코드\n",
        "## train batch를 받가 DQN이 학습할수(update) 있게 전처리 하고 학습 결과(loss)를 반환한다.\n",
        "ㆍ np.vstack : array를 세로로 붙이기\n",
        "![대체 텍스트](https://t1.daumcdn.net/cfile/tistory/9909EF345D53F4D916)\n",
        "\n",
        "ㆍ Target Q를 구하기 \n",
        "- y_batch 는 reward + discount 팩처 * maxQ 아니면 게임종료(0)\n",
        "\n",
        "ㆍ DQN.update(X_batch, y_batch) \n",
        "- X_batch는 현재 state\n",
        "- Y_batch는 TargetQ 값\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dTKgZhHBfj8l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_minibatch(DQN: DQN, train_batch: list) -> float:\n",
        "    \"\"\"Prepare X_batch, y_batch and train them\n",
        "    Recall our loss function is\n",
        "        target = reward + discount * max Q(s',a)\n",
        "                 or reward if done early\n",
        "        Loss function: [target - Q(s, a)]^2\n",
        "    Hence,\n",
        "        X_batch is a state list\n",
        "        y_batch is reward + discount * max Q\n",
        "                   or reward if terminated early\n",
        "    Args:\n",
        "        DQN (dqn.DQN): DQN Agent to train & run\n",
        "        train_batch (list): Minibatch of Replay memory\n",
        "            Eeach element is a tuple of (s, a, r, s', done)\n",
        "    Returns:\n",
        "        loss: Returns a loss\n",
        "    \"\"\"\n",
        "    state_array = np.vstack([x[0] for x in train_batch])\n",
        "    action_array = np.array([x[1] for x in train_batch])\n",
        "    reward_array = np.array([x[2] for x in train_batch])\n",
        "    next_state_array = np.vstack([x[3] for x in train_batch])\n",
        "    done_array = np.array([x[4] for x in train_batch])\n",
        "\n",
        "    X_batch = state_array\n",
        "    y_batch = DQN.predict(state_array)\n",
        "\n",
        "    Q_target = reward_array + DISCOUNT_RATE * np.max(DQN.predict(next_state_array), axis=1) * ~done_array\n",
        "    y_batch[np.arange(len(X_batch)), action_array] = Q_target\n",
        "\n",
        "    # Train our network using target and predicted Q values on each episode\n",
        "    loss, _ = DQN.update(X_batch, y_batch)\n",
        "\n",
        "    return loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iE4vU9U-f2tX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def bot_play(mainDQN: DQN) -> None:\n",
        "    \"\"\"Runs a single episode with rendering and prints a reward\n",
        "    Args:\n",
        "        mainDQN (dqn.DQN): DQN Agent\n",
        "    \"\"\"\n",
        "    state = env.reset()\n",
        "    total_reward = 0\n",
        "\n",
        "    while True:\n",
        "        env.render()\n",
        "        action = np.argmax(mainDQN.predict(state))\n",
        "        state, reward, done, _ = env.step(action)\n",
        "        total_reward += reward\n",
        "        if done:\n",
        "            print(\"Total score: {}\".format(total_reward))\n",
        "            break"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0LGKtcz-uWPr",
        "colab_type": "text"
      },
      "source": [
        "#  ε-Greedy or Annealiing epsilon\n",
        "## ε를 구하는 다른 방법\n",
        "시간이 지날수록 ε이 작아진다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1x0F74k5ieLh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def annealing_epsilon(episode: int, min_e: float, max_e: float, target_episode: int) -> float:\n",
        "    \"\"\"Return an linearly annealed epsilon\n",
        "    Epsilon will decrease over time until it reaches `target_episode`\n",
        "         (epsilon)\n",
        "             |\n",
        "    max_e ---|\\\n",
        "             | \\\n",
        "             |  \\\n",
        "             |   \\\n",
        "    min_e ---|____\\_______________(episode)\n",
        "                  |\n",
        "                 target_episode\n",
        "     slope = (min_e - max_e) / (target_episode)\n",
        "     intercept = max_e\n",
        "     e = slope * episode + intercept\n",
        "    Args:\n",
        "        episode (int): Current episode\n",
        "        min_e (float): Minimum epsilon\n",
        "        max_e (float): Maximum epsilon\n",
        "        target_episode (int): epsilon becomes the `min_e` at `target_episode`\n",
        "    Returns:\n",
        "        float: epsilon between `min_e` and `max_e`\n",
        "    \"\"\"\n",
        "\n",
        "    slope = (min_e - max_e) / (target_episode)\n",
        "    intercept = max_e\n",
        "\n",
        "    return max(min_e, slope * episode + intercept)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "96kjH6fLxQos",
        "colab_type": "text"
      },
      "source": [
        "# Main 함수\n",
        "ㆍ MAX_EPISODE 만큼 훈련한다. \n",
        "\n",
        "ㆍ ε가 작으면 무작위 선택, 크면 Q에서 주는 값으로 행동한다.\n",
        "\n",
        "ㆍ 선택된 액션을 하고, 다음 스테이트, 보상, 종료여부를 받는다.\n",
        "\n",
        "ㆍ 게임이 끝나면 보상을 -1로 \n",
        "\n",
        "ㆍ 아니면 replay memory에 상태, 행동, 보상, 다음상태, 종료여부를 넣는다\n",
        "\n",
        "ㆍ 선택된 액션을 하고, 다음 스테이트, 보상, 종료여부를 받는다.\n",
        "\n",
        "ㆍ 훈련할 데이터를 Batch size만큼 모으고, 그중 무작위 선택하여 DQN을 학습한다.\n",
        "\n",
        "ㆍ 훈련된 모델을 저장한다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z2db2FJkhYkj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def main():\n",
        "    print(\"Start\")\n",
        "    # store the previous observations in replay memory\n",
        "    replay_buffer = deque(maxlen=REPLAY_MEMORY)\n",
        "    last_100_game_reward = deque(maxlen=100)\n",
        "\n",
        "    with tf.Session() as sess:\n",
        "        mainDQN = DQN(sess, INPUT_SIZE, OUTPUT_SIZE)\n",
        "        init = tf.global_variables_initializer()\n",
        "        sess.run(init)\n",
        "\n",
        "        for episode in range(MAX_EPISODE):\n",
        "            #e = 1./(( episode / 10) + 1)\n",
        "            e = annealing_epsilon(episode, MIN_E, 1.0, EPSILON_DECAYING_EPISODE)\n",
        "            done = False\n",
        "            state = env.reset()\n",
        "\n",
        "            step_count = 0\n",
        "            while not done:\n",
        "\n",
        "                if np.random.rand() < e:\n",
        "                    action = env.action_space.sample()\n",
        "                else:\n",
        "                    action = np.argmax(mainDQN.predict(state))\n",
        "\n",
        "                next_state, reward, done, _ = env.step(action)\n",
        "\n",
        "                if done:\n",
        "                    reward = -1\n",
        "\n",
        "                replay_buffer.append((state, action, reward, next_state, done))\n",
        "\n",
        "                state = next_state\n",
        "                step_count += 1\n",
        "\n",
        "                if len(replay_buffer) > BATCH_SIZE:\n",
        "                    minibatch = random.sample(replay_buffer, BATCH_SIZE)\n",
        "                    train_minibatch(mainDQN, minibatch)\n",
        "\n",
        "            print(\"[Episode {:>5}]  steps: {:>5} e: {:>5.2f}\".format(episode, step_count, e))\n",
        "\n",
        "            # CartPole-v0 Game Clear Logic\n",
        "            last_100_game_reward.append(step_count)\n",
        "            if len(last_100_game_reward) == last_100_game_reward.maxlen:\n",
        "                avg_reward = np.mean(last_100_game_reward)\n",
        "                if avg_reward > 199.0:\n",
        "                    print(\"Game Cleared within {} episodes with avg reward {}\".format(episode, avg_reward))\n",
        "                    break\n",
        "        mainDQN.save(\"dqn_v1\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_v5XyC5201sO",
        "colab_type": "text"
      },
      "source": [
        "## Main 함수실행"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qLg2Q588j0YX",
        "colab_type": "code",
        "outputId": "f011ce97-ee13-47b8-fd48-b5f30848b6a8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 377
        }
      },
      "source": [
        "main()"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Start\n",
            "WARNING:tensorflow:From <ipython-input-6-24523b26e595>:48: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.Dense instead.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/layers/core.py:187: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `layer.__call__` method instead.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/losses/losses_impl.py:121: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "[Episode     0]  steps:    10 e:  1.00\n",
            "[Episode     1]  steps:    10 e:  0.00\n",
            "[Episode     2]  steps:     9 e:  0.00\n",
            "[Episode     3]  steps:    10 e:  0.00\n",
            "[Episode     4]  steps:    21 e:  0.00\n",
            "[Episode     5]  steps:    10 e:  0.00\n",
            "[Episode     6]  steps:     9 e:  0.00\n",
            "[Episode     7]  steps:     9 e:  0.00\n",
            "[Episode     8]  steps:    10 e:  0.00\n",
            "[Episode     9]  steps:     9 e:  0.00\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6DT-5gec06kB",
        "colab_type": "text"
      },
      "source": [
        "# DQN 결과 보기\n",
        "## 봉이 쓰러지지 않고 잘 서있으면 성공, 아니면 실패"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "od8i_tPU4fQ7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from pyvirtualdisplay import Display\n",
        "Display().start()\n",
        "\n",
        "import gym\n",
        "from IPython import display\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "init = tf.global_variables_initializer()\n",
        "env = gym.make('CartPole-v0')\n",
        "state = env.reset()\n",
        "img = plt.imshow(env.render('rgb_array')) # only call this once\n",
        "tf.reset_default_graph()\n",
        "with tf.Session() as sess:\n",
        "\n",
        "    mainDQN = DQN(sess, INPUT_SIZE, OUTPUT_SIZE)\n",
        "    init = tf.global_variables_initializer()\n",
        "    mainDQN.load(\"dqn_v1\")\n",
        "    for _ in range(100):\n",
        "        img.set_data(env.render('rgb_array')) # just update the data\n",
        "        display.display(plt.gcf())\n",
        "        display.clear_output(wait=True)\n",
        "        #action = env.action_space.sample()\n",
        "        action = np.argmax(mainDQN.predict(state))\n",
        "        print()\n",
        "        next_state, reward, done, _ = env.step(action)\n",
        "        state = next_state\n",
        "        print(\"action, state : {0} {1}\".format(action,state ))\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}